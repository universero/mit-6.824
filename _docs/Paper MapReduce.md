>[论文地址]([rfeet.qrk](https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf))
___
## 概述

MapReduce: Simplified Data Processing on Large Clusters 是Google发的一篇关于分布式计算的论文, 作者是Jeffrey Dean 和 Sanjay Ghemawat.
下文主要是对原文的翻译, 以及部分自己的理解(部分不重要的内容会进行删减)
## Abstract

MapReduce是一种用于处理和生成海量数据的编程范式和相关实现. 包括*map*和*reduce*两种function
- *map*: 处理输入kv对来生成一系列中间kv对
- *reduce*: 合并所有的内部kv对
这个模型可以处理许多现实世界中的任务

依照这种函数式风格编写的程序会在一个大集群中自动的并行执行. 运行时系统会关注输入数据的分区, 机器上程序执行的调度, 集群错误处理和机器间通信. 这使得没有任何并行或分布式系统开发经验的也能简单的利用大型分布式系统的资源
## 1 Introduction

在过去五年中, 作者和谷歌团队实现了大量具有特殊用途的大数据计算, 例如爬取的文档, 网站请求日志等, 为了计算多种类型的派生数据(如反向索引, 不同表示方式的web文档图, 每个站点上怕渠道的页面数量总和, 单日高频访问请求等). 大多数计算概念上是直接的, 但是输入数据的规模通常很大, 所以计算需要分配到成千上万的机器上以在一个合理的时间内得到结果. 原本简单的计算被大量的如何并行计算、分配数据、处理错误的复杂代码所掩盖

为了专注于计算逻辑, 我们设计了一个新的抽象来隐藏并行计算、错误容忍、数据分配和负载均衡的复杂度. 受到Lisp和许多其他函数式语言中map和reduce的启发, 我们意识到大多数的计算包含了对于输入的逻辑记录执行map操作以计算中间kv对, 以及对所有共享key的value执行reduce操作来组合出合适的派生数据. 这一函数式模型使得用户定义的map和reduce操作能简单地实现大规模的并行计算并将重新执行作为错误容忍的主要机制

这篇文章最主要的共享是简单但有力的接口, 这些接口能够允许自动并行计算和大规模计算调度, 配合这些接口的实现能在大规模商品级机器上实现高性能计算

Section2描述了基本的程序模型并给出了一些例子, Section3描述了一个为我们基于集群的计算环境定制的MapReduce接口的实现, Section 4 描述了一些我们认为有用的模型精化, Section 5 呈现了我们的实现在不同任务上的性能测量. Section 6 描述了在Google中对MapReduce使用的探索, 包括我们在重写产品索引系统的经验. Section 7 讨论了相关的和未来的工作
由于只是为了完成Lab, 所以这里偷了点懒, 有些不重要的section会跳过去

## 2 Programming Model

计算任务将一组kv对作为输入并产生一系列kv对作为输出, MapReduce库的用户需要用map和reduce两种方法表示计算任务

Map, 接收一个输入的pair并产生一系列的中间kv对. MapReduce  library 将所有的与中间key *I* 关联的value分组到一起

Reduce, 接受一个中间key *I*和这个key对应的一组值, 然后将他们合并到一起形成可能更小的一组value. 比较典型地, 每次Reduce调用会产生0或1个输出值. 内部值通过一个迭代器提供给reduce, 这使得我们可以处理超出内存大小的一组值.
### 2.1 Example

问题: 统计一大批文档中每个单次出现的次数
用户可能写出下面这样的pseudo-code(伪代码)
```c
map(String key, String value):
	// key: document name
	// value: document contents
	for each word w in value:
		EmitIntermediate(w, "1");
reduce(String key, Iterator values):
	// key: a word
	// values: a list of counts
	int result = 0;
	for each v in values:
		result += ParseInt(v);
	Emit(AsString(result));
```
map方法emit(发出, 但是翻译过来有点怪)每个单词并加上其出现次数统计(例子中都用1). reduce方法将所有emit的单词出现次数求和
此外, 用户编写代码用输入名、输出文件以及可选协调参数来填充mapreduce specification对象. 用户可以通过传递specification对象执行MarReduce函数.
### 2.2 Type

尽管先前的伪代码用字符串作为输入输出, 概念上map和reduce都有对应的类型
- map(k1, v1) -> lilst(k2, v2)
- reduce (k2, list(v2)) -> list(v2)
## 2.3 More Example

给出了更多能用MapReduce完成计算的例子
- Distributed Grep
- Count of URL Access Frequency
- Reverse Web-Link Graph
- Term-Vector pre Host
- Inverted Index
- Distributed Sort
## 3 Implementation

MapReduce interface有许多的实现, 正确的选择取决于环境, 例如一个实现可能适合小型内存共享机器, 另一个实现适合大型NUMA多处理器机器, 还有的适合通过网络连接的机器集群. 这一节描述了的在Google被广泛使用的实现: 通过以太网互联的大型商品级PC集群. 在这个环境中
- 机器通常是X86处理器, Linux系统, 2-4G内存
- 使用商品网络硬件, 机器层面大约100mb/s或1gb/s, 但整体带宽要低得多
- 一个集群中有成百上千的机器, 机器故障是很常见的
- 由直接附加到机器上的廉价IDE硬盘提供存储. 由Google内部的分布式文件系统管理磁盘
- 用户将工作提交给调度系统, 每个工作包含一系列任务, 由调度器分配给合适的机器.
### 3.1 Execution Overview

通过将输入数据划分成M份, 来将Map调用在多台机器间分配. 输入分片可以被不同的机器并行处理. 通过利用分区函数将中间key划分成R份, 来分配Reduce调用. partition(R)的数量和分区函数都有用户定义.
下图呈现了MapReduce操作的概述.
![Execution Overview](Execution%20Overview.png)
当用户调用了MapReduce function时, 会发送如下的过程
- 用户程序中的MapReduce库首先会将输入文件切分为M片, 每片大约16MB到64MB(由用户通过配置参数管理), 然后在一批机器上启动程序的拷贝
- 其中一个拷贝是特殊的, 作为master存在, 剩余的则是被master分配任务的worker. 总共有M个map任务和R个reduce任务等待被分配, master会挑选空闲的worker分配一个任务
- 一个被分配了map任务的worker从相应的切片中读取输入, 解析并处理输入的kv对, 产生的中间kv对会缓存在内存中
- 内存中的kv对会阶段性的存储到本地磁盘中, 然后被partitioning function切分为R个区域. 本地磁盘中存储kv对的位置会发回给master, 并由master将这些地址分发给负责reduce的worker
- 当reduce worker收到了master通知的地址后, 通过RPC来从map worker中读取这些缓存的数据. 当一个reduce worker读取了所有的中间数据后, 它会将所有的中间key排序使得相同的key被分组到一起. 通常不同的key会被映射到同一个reduce task上, 所有排序是必要的. 且当数据大于内存时外排序也是必要的
- reduce worker遍历排序后的中间结果, 对于每个不同的中间key, 它将这个key和对于的中间value传递给Reduce function. Reduce function的输出会被附加到最后的输出文件后
- 当所有的map任务和reduce任务都完成后, master会唤醒用户程序, 在此时用户程序中的MapReduce调用会return到用户代码中
在成功完成后, 最终的输出会存储在R个输出文件中(每个reduce task任务一个). 通常用户不需要合并这些文件, 他们通常会作为下一个MapReduce调用或者其他分布式应用的输入
### 3.2 Master Data Structures

master维护一些数据结构, 对于每个map 任务和reduce任务, 它会存储对应的状态(idle, in-progress or completed), 以及对应worker的标识

master是将中间结果从map任务传递到reduce任务的管道. 所以, 对于每个完成的map任务, master会存储R中间文件区域的大小和位置, 每当收到一个map tasks任务完成都会更新文件大小和位置, 并将信息增量推送给正在工作的reduce-tasks
### 3.3 Fault Tolerance

MapReduce需要在成千上万的机器上处理大量的数据, 所以MapReduce必须优雅的容忍机器错误
#### Worker Failure

master会周期性的ping每一个worker, 如果在一定时间内没有收到这个worker的响应, master会将这个worker标记为failed. 任意一个map tasks被worker完成后都会将被重置为idle状态, 然后允许被调度到其他机器上. 类似地, 任何一个进行中的task失败后, 也会被重置为idle并允许被重新调度

当错误发生时, 完成过的map任务也会重新被执行, 因为他们的输出被存储在那个出错的机器的本地磁盘上, 没法被访问. 完成过的reduce任务不需要重新执行, 因为他们的结果已经存储到了全局的文件系统中

当一个map任务一开始被worker A执行, A出错后再被worker B执行时, 所有执行reduce任务的worker都会被通知这个重新执行, 任何一个还没有从A读取数据的将会从B读取数据

MapReduce对于一个大规模的worker failures是有可恢复性的. 例如在MapReduce操作时, 在运行中集群上的网络维护导致了80个机器同时的不可用, master将会重新调度失败的任务并继续后续的操作知道操作完成.
#### Master Failure

master周期性的记录其维护的数据结构的checkpoints, 当master出错后, 一个新的拷贝可以从最近的checkpoint恢复. 然后, 鉴于只有一个master, 他的故障是不可能的, 所以我们的实现会在master出错时中断MapReduce计算. Clients可以检测它的状态并尝试重新进行MapReduce操作
#### Semantics in the Presence of Failures

当用户提供的map和reduce算子是对相同输入有确定性的函数时, 我们的分布式实现会产生和无错顺序执行相同的输出

我们依靠map和reduce任务输出的原子性的提交来实现这个特性. 每个进行中的任务将它的输出写到私有的临时文件中. 一个Reduce任务产生一个这样的文件, map任务会产生R个这样的文件. 当map任务完成时, workers会发送消息到master并在其中包括R个临时文件的名称. 如果master受到了一个已完成的map任务的完成消息, 则会忽略, 否则则是将R文件的名称记录到数据结构中

当一个reduce任务完成时, reduce worker会原子性的重命名这个临时输出文件为最终输出文件. 如果相同的reduce任务在多个机器上执行, 多次的rename调用会在同一个文件上执行. 我们依靠底层文件系统提供的原子性的重命名操作来保证最终文件系统只有一个reduce任务执行产生的文件

绝大多数的map和reduce操作是确定的, 在这种情况下我们的semantics(语法?)与顺序执行是等价, 因此这使得开发者可以简单地解释程序的行为.当map或reduce是不确定时, 我们提供的了更弱但是仍可解释的语法. 当不确定的算子出现时, 某个任务R1的输出与R1顺序执行不确定程序的输出是一致的. 但是, 不同的reduce任务R2可能相应的产生不同于顺序执行的不确定性程序.
### 3.4 Locality

网络带宽是我们的计算环境中相对缺乏的资源, 我们通过利用输入数据(由GFS管理)存储在组成集群的机器的本地磁盘上这一特点来节省网络带宽. GFS将每个文件切分为64MB大小的块, 并且每个块有多个不同的备份存储在不同的机器上. master会考虑输入文件的位置信息并尽可能的将map任务调度到具有相应输入数据的本地备份的机器上. 如果不存在这样的机器也会试图调度到离有备份机器尽可能近的机器上(如同一个网络交换机). 当MapReduce在一个足够到的集群上运行时, 大多数数据都是本地传输而不占用比较多的网络带宽
### 3.5 Task Granularity

先前提到, 我们将map阶段拆分为M块, Reduce阶段拆分成R块. 理想情况下, M和R应该比worker的数量大得多, 让每个worker执行许多不同的任务来提高负载均衡以及加快故障恢复的速度.

在我们的实现中M和R的大小有实际的边界, 因为master必须完成O(M+R)的调度决定和在内存中维护O(M\*R)的状态

此外, R还被用户约束, 因为每个reduce任务的输出会存放在一个单独的输出文件中. 在实际中, 我们倾向于选择M使得每个单独任务大约在16MB到64MB, 并且我们让R是我们能用的机器的比较小的倍数.
### 3.6 Backup Tasks

导致MapReduce操作时间过长的很大一部分原因是straggler(掉队者)在一个任务上花费了过长的时间, 原因可能多种多样(如配置较差, 网络带宽不足等)
为了解决这个问题, 我们在MapReduce快结束时, 对剩余的任务进行备份执行, 当备份和原任务中有一个完成就将这个任务标记为完成, 这样只增加了很少的一部分计算资源, 但却大幅减少了掉队者的影响
## 4 Refinements

尽管map和reduce就足够应付大部分场景了, 但是我们还发现了许多可能有用的扩展
### 4.1 Partition Function

数据根据中间key的Partition Function被分到不到的Reduce文件, 默认的都是使用hash函数, 这样获取到的较为均匀, 但是有时候使用不同的函数会更有用, 比如对于URL希望同一个hostname的分到一起就可以用hash(Hostname(urlKey))
### 4.2 Ordering Guarantees

我们保证在给定分区内，中间键/值对按键递增顺序处理。
这种排序保证可以很容易地为每个分区生成排序的输出文件，当输出文件格式需要支持有效的按键随机访问查找时，或者输出的用户发现对数据进行排序很方便时，这是很有用的。
### 4.3 Combiner Function

在某些情况下，每个map任务产生的中间键存在明显的重复，并且用户指定的Reduce函数是可交换的和关联的。
第2.1节中的单词计数就是一个很好的例子。由于单词频率倾向于遵循Zipf分布，因此每个map任务将生成成百上千条< 1,1 >形式的记录。所有这些计数将通过网络发送到单个reduce任务，然后由reduce函数将它们加在一起生成一个数字。我们允许用户指定一个可选的Combiner函数，该函数在数据通过网络发送之前对其进行部分合并。
Combiner函数在每台执行映射任务的机器上执行。通常使用相同的代码来实现combiner和reduce函数。reduce函数和组合函数之间的唯一区别是MapReduce库如何处理函数的输出。reduce函数的输出被写入最终的输出文件。
组合函数的输出被写入中间文件，该中间文件将被发送给reduce任务。
部分组合显著加快了某些类型的MapReduce操作。
附录A包含一个使用组合器的示例。
### 4.4 Input and Output Types

MapReduce支持不同类型的输入输出. text模式输入将每行视为一个键/值对：键是文件中的偏移量，值是该行的内容。
另一种常见的支持格式存储按键排序的键/值对序列。每个输入类型实现都知道如何将自己分割成有意义的范围，以便作为单独的map任务进行处理（例如，文本模式的范围分割确保范围分割只发生在行边界）。用户可以通过提供简单的Reder接口的实现来添加对新输入类型的支持，尽管大多数用户只使用少数预定义输入类型中的一种。
读取器不一定需要提供从文件读取的数据。例如，很容易定义从数据库或从内存中映射的数据结构中读取记录的读取器。
以类似的方式，我们支持一组输出类型来生成不同格式的数据，并且用户代码很容易添加对新输出类型的支持。
### 4.5 Side-effects

在某些情况下，MapReduce 的用户发现生成辅助文件作为其map或reduce操作的额外输出会比较方便。我们依靠应用程序writer确保此类副作用是原子且幂等的。通常，应用程序会将数据写入临时文件，并在完全生成后原子性地重命名该文件。我们不为单个任务生成的多个输出文件的原子两阶段提交提供支持。因此，那些需要满足跨文件一致性的多个输出文件生成任务必须是确定性的。这一限制在实际应用中从未成为问题。
### 4.6 Skipping Bad Records

有时，用户编写的代码中会存在一些错误，这些错误会导致 Map 或 Reduce 函数在某些记录上出现确定性的崩溃。这类错误会致使 MapReduce 操作无法完成。通常的做法是修复这个错误，但有时这并不可行；也许错误存在于第三方库中，而该库的源代码无法获取。此外，有时忽略一些记录也是可以接受的，比如在对大型数据集进行统计分析时。我们提供了一种可选的执行模式，在这种模式下，MapReduce 库能够检测出导致确定性崩溃的记录，并跳过这些记录以实现向前推进。

每个工作进程都会安装一个信号处理程序，用于捕获分段违规和总线错误。在调用用户自定义的映射或归约操作之前，MapReduce 库会将参数的序列号存储在一个全局变量中。如果用户代码引发了信号，信号处理程序会向 MapReduce 主机发送一个“last gasp”型 UDP 数据包，其中包含序列号。当主机在某一特定记录上检测到多次失败时，它会指示在发出对应 Map 或 Reduce 任务的下一次重新执行指令时应跳过该记录。
### 4.7 Local Execution

Map或Reduce函数中的调试问题可能会很棘手，因为实际的计算发生在分布式系统中，通常在数千台机器上，由主机动态地做出工作分配决策。为了方便调试、分析和小规模测试，我们开发了MapReduce库的另一种实现，它在本地机器上顺序地执行MapReduce操作的所有工作。控件提供给用户，这样计算就可以限制在特定的Map任务上。用户用一个特殊的标志来调用他们的程序，然后可以很容易地使用他们认为有用的任何调试或测试工具（例如gdb）。
### 4.8 Status Information

master运行一个内部HTTP服务器，并导出一组状态页供人们使用。状态页显示计算的进度，例如有多少任务已经完成，有多少任务正在进行中，输入字节数，中间数据字节数，输出字节数，处理速率等。这些页面还包含到每个任务生成的标准错误和标准输出文件的链接。用户可以使用这些数据来预测计算需要多长时间，以及是否应该在计算中添加更多的资源。这些页面还可以用来确定计算何时比预期慢得多。此外，顶级状态页显示哪些worker失败了，以及他们失败时正在处理哪些map和reduce任务。当试图诊断用户代码中的错误时，此信息非常有用。
### 4.9 Counters

MapReduce库提供了一个计数器工具来计算各种事件的发生次数。例如，用户代码可能想要计算处理的单词总数或索引的德语文档的数量等。为了使用此功能，用户代码创建一个命名计数器对象，然后在Map和/或Reduce函数中适当地增加计数器。来自各个worker的计数器值定期传播到master（在ping响应的基础上）。master从成功的map和reduce任务中聚合计数器值，并在MapReduce操作完成时返回给用户代码。当前计数器的值也显示在主状态页面上，以便人们可以观看实时计算的进度。当聚合计数器值时，master消除了重复执行相同map或reduce任务的影响，以避免重复计数。（重复执行可能是由于我们使用备份任务和由于失败而重新执行任务造成的。）一些计数器值由MapReduce库自动维护，例如处理的输入键/值对的数量和产生的输出键/值对的数量。用户发现计数器功能对于检查MapReduce操作的行为非常有用。例如，在一些MapReduce操作中，用户代码可能希望确保生成的输出对的数量恰好等于处理的输入对的数量，或者处理的德语文档的比例在处理的文档总数的可容忍的比例范围内。
## Performance

这一节测量了MapReduce在大机器上两个计算任务的性能, 和完成lab影响不大, 跳过.

后续的章节都差不多, 暂时都先跳过
